{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "gather": {
          "logged": 1745923197082
        }
      },
      "source": [
        "# Watermark Robustness Pipeline\n",
        "\n",
        "This notebook implements a pipeline to:\n",
        "- Download raw images from Azure Blob Storage\n",
        "- Add watermarks to the raw images\n",
        "- Apply a variety of image transformations to those watermarkedimages\n",
        "- See if watermarks are still detectable in both original and transformed images\n",
        "- Calculate similarity and other metrics\n",
        "- Save results to CSV files for further analysis\n",
        "\n",
        "This Jupyter notebook uses the azureml_py38 environment (from the requirements_watermark.txt file).\n",
        "It is also designed to be run in an \"Azure AI\" environent (https://ml.azure.com/).\n",
        "\n",
        "We recommend reading the comments before running each section. Do not just click \"Run all\" - not all of the code sections are needed.\n",
        "\n",
        "In particular we have 'hard-coded' some of the file locations.\n",
        "These should be similar when you install the repo to your file system inside Azure AI. However you will have to change \"David.Fletcher\" to your name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scratch notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "## There are a collection of scripts in the repo which may be useful for the reader.\n",
        "#\n",
        "#\n",
        "#                \"StripBits\" creates the BitMasks in the 'Output/Mask' folders.\n",
        "#                \"StabSig\" (an ipynb file!) creates the transforms in the \n",
        "#                                       'OutputTransformationsStableSig' folder.\n",
        "#                \"scratchfile\" creates the two missing transforms from StabSig.\n",
        "#                \"test\" creates the transforms in the 'OutputTransformations' folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Control Variables\n",
        "\n",
        "This is a list of control variables referenced elsewhere in the Pipeline.\n",
        "The Pipeline is currently set up to target an Azure AI environment.\n",
        "\n",
        "We strongly recommend changing at least the user_name control variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### CHOOSE WATERMARK DETECTION METHOD #####\n",
        "\n",
        "## The following code overwrites the \"detect_watermark\" function with the relevant detector from one of the three methods.\n",
        "## This means that we have to change the WatermarkMethod variable to switch between methods (and have to run the pipeline multiple times).\n",
        "## Methods available: Stable Signature, Watermark-anything, TrustMark\n",
        "\n",
        "WatermarkMethod = \"Watermark_Anything\" # Methods are Stable_Signature, Watermark_Anything, TrustMark  TESTED WITH HARDCODED\n",
        "#WatermarkMethod = \"TrustMark\" # Methods are Stable_Signature, Watermark_Anything, TrustMark           TESTED WITH HARDCODED\n",
        "#WatermarkMethod = \"Stable_Signature\" # Methods are Stable_Signature, Watermark_Anything, TrustMark     Broken - \"from hidden import attenuation\" error.\n",
        "                                                                                                        #May need StableSignature/hidden to be on the path. Or Detector. Ask Jireh to verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Inside Ofcom Azure AI you will generally need to change 'David.Fletcher' to your username.\n",
        "# When running locally or in a different environment, modify the directory structure accordingly.\n",
        "user_name = 'David.Fletcher' # Used to create the home directory and various child directories.\n",
        "azure_root_dir = '/home/azureuser/cloudfiles/code/Users/' # Azure AI shared root directoty. Used to create the home directory.\n",
        "root_dir = azure_root_dir + user_name # Root directory. Data directory and code directory are subdirectories of this.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "## File locations\n",
        "# We are currently storing the raw and transformed images at several locations spread throughout the Azure AI file system.\n",
        "# (This is because several team members worked on different parts of the project, and moving files inside Azure AI is non-trivial).\n",
        "# As such we will include the current file locations in Azure AI, and control variables to set up a proper directory structure.\n",
        "\n",
        "Use_Current_File_Locations = True # Set to False to use the new directory structure.\n",
        "\n",
        "if Use_Current_File_Locations:\n",
        "    # Current file locations in Azure AI\n",
        "    home_dir = '/home/azureuser/cloudfiles/code/Users/David.Fletcher/ost-embedding-research/' # Home directory for the project.\n",
        "    raw_images_dir = '/home/azureuser/cloudfiles/code/Users/David.Fletcher/embedding_data/raw_watermarked_images/' #Location of the images downloaded from BLOB storage\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    trustmark_test_location = '/home/azureuser/cloudfiles/code/Users/David.Fletcher/embedding_data/trustmark/' # Location used for the TrustMark example run: not part of the main pipeline.\n",
        "\n",
        "\n",
        "# The raw images are current stored in the Azure AI file system at '/home/azureuser/cloudfiles/code/Users/David.Fletcher/embedding_data/raw_watermarked_images/'\n",
        "# Here is code to generate a \n",
        "\n",
        "if (Use_Current_File_Locations == False):\n",
        "    home_dir = root_dir + '/ost-embedding-research/'\n",
        "    raw_images_dir = root_dir + '/embedding_data/raw_watermarked_images/'\n",
        "\n",
        "    # We may be using the Watermark-Anything library. The code will swap the working directory at that point.\n",
        "    wk_dir_wa = root_dir + '/ost-embedding-research/watermarkanything/watermark-anything'\n",
        "    wk_dir_wa_checkpoints =  wk_dir_wa + \"/checkpoints\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    trustmark_test_location = root_dir + '/embedding_data/trustmark/'\n",
        "\n",
        "# TO DO\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "##\n",
        "\n",
        "\n",
        "# We have included code to watermark and detect a watermark for the TrustMark method.\n",
        "# This is not part of the pipeline, but included to help the reader.\n",
        "# The following variable controls whether that code is run.\n",
        "# This variable is only used if WatermarkMethod is set to \"TrustMark\".\n",
        "TrustMark_Example_Run = False\n",
        "\n",
        "\n",
        "# We have included code to download the raw images from BLOB storage.\n",
        "# This should not be done unless you want to download the images again, and is disabled by default.\n",
        "DOWNLOAD_IMAGES = False # Set to True to download the images from BLOB storage.\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "## Global variable to set the number of images to process\n",
        "images_to_process = 2 #700 is the max value,\n",
        "images_to_ = images_to_process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CWD set to: /mnt/batch/tasks/shared/LS_root/mounts/clusters/df-cpu/code/Users/David.Fletcher/ost-embedding-research/PotentialTransforms\n"
          ]
        }
      ],
      "source": [
        "# Set working directory to where the configs are expected.\n",
        "wk_dir = azure_root_dir + user_name + '/ost-embedding-research/PotentialTransforms' \n",
        "os.chdir(wk_dir)\n",
        "print(\"CWD set to:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook is designed to run three different watermark detection methods.\n",
        "Comment in the relevant method in the following cell.\n",
        "This notebook does not support running multiple methods at once - please run the notebook multiple times for that.\n",
        "\n",
        "The process of downloading raw images from our shared BLOB storage is not strictly part of this pipeline. The code for that is in the \"access_blob_storage_subfolder.py\" script in this folder (/PotentialTransforms). For testing purposes we have included a stripped down version of it in this notebook.\n",
        "\n",
        "The actual creation of watermarked images is not part of this pipeline, as it was handled manually.\n",
        "\n",
        "The code to create the TrustMark-watermarked images is in the \"trustmark_test.py\" script, at /ost-embedding-research/trustmark/trustmark_test.py\n",
        "The code to create the WatermarkAnything-watermarked images is at /ost-embedding-research/watermarkanything, in the inferencehacking copy.ipynb and associated scripts.\n",
        "\n",
        "## Jireh - please verify the following Stable Signature statement;\n",
        "The code to create Stable Signature-watermarked images is in the /utils/ folder, in the \"generate_watermark_imgs.py\" file.\n",
        "\n",
        "The files stored in Azure BLOB storage are watermarked. [If not, please flag where the watermarked files are stored]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if Use_Current_File_Locations:\n",
        "\n",
        "    if WatermarkMethod == \"Watermark_Anything\":\n",
        "        ## Watermark-anything watermarks and image locations\n",
        "        download_path = \"/home/azureuser/cloudfiles/code/Users/David.Fletcher/embedding_data/watermarked_transforms_wa/\"\n",
        "        raw_images_path = \"/home/azureuser/cloudfiles/code/Users/Jireh.Jam/Deepfake-Embedding-Mitigation-Measures/watermark-anything/outputs/watermarked/\"\n",
        "        sub_folder = \"watermarked/\"   \n",
        "        wk_dir_wa = '/home/azureuser/cloudfiles/code/Users/David.Fletcher/ost-embedding-research/watermarkanything/watermark-anything'\n",
        "        wk_dir_wa_checkpoints = wk_dir_wa + \"/checkpoints\"\n",
        "        \n",
        "    if WatermarkMethod == \"TrustMark\":\n",
        "        ## Trustmark watermarks and image locations\n",
        "        download_path = \"/home/azureuser/cloudfiles/code/Users/H.Kristjansdottir/watermarked_transforms_tm/\"\n",
        "        raw_images_path = \"/home/azureuser/cloudfiles/code/Users/H.Kristjansdottir/trustmark_watermarked/\"\n",
        "        sub_folder = \"trustmark_watermarked/\"\n",
        "\n",
        "    if WatermarkMethod == \"Stable_Signature\":\n",
        "        ## Stable Signature watermarks and image locations??\n",
        "        download_path = \"/home/azureuser/cloudfiles/code/Users/David.Fletcher/embedding_data/\"\n",
        "        sub_folder = \"raw_watermarked_images/\"   \n",
        "        raw_images_path = download_path + sub_folder #Must be a subdirectory for download dependency reasons.\n",
        "\n",
        "if (Use_Current_File_Locations == False):\n",
        "    home_dir = root_dir + '/ost-embedding-research/'\n",
        "\n",
        "    if WatermarkMethod == \"Watermark_Anything\":\n",
        "        ## Watermark-anything watermarks and image locations\n",
        "        download_path = root_dir + \"/embedding_data/watermarked_transforms_wa/\"\n",
        "        raw_images_path = home_dir + \"/watermark-anything/outputs/watermarked/\" #Sub-optimal, but matches the current directory structure.\n",
        "        sub_folder = \"watermarked/\"   \n",
        "        wk_dir_wa = home_dir + '/watermarkanything/watermark-anything'\n",
        "        wk_dir_wa_checkpoints = wk_dir_wa + \"/checkpoints\"\n",
        "        \n",
        "    if WatermarkMethod == \"TrustMark\":\n",
        "        ## Trustmark watermarks and image locations\n",
        "        download_path = root_dir + \"/watermarked_transforms_tm/\"\n",
        "        raw_images_path = \"/trustmark_watermarked/\" #Sub-optimal, but matches the current directory structure.\n",
        "        sub_folder = \"trustmark_watermarked/\"\n",
        "\n",
        "    if WatermarkMethod == \"Stable_Signature\":\n",
        "        ## Stable Signature watermarks and image locations??\n",
        "        download_path = root_dir + \"/embedding_data/\"\n",
        "        sub_folder = \"raw_watermarked_images/\"   #Sub-optimal, but matches the current directory structure.\n",
        "        raw_images_path = download_path + sub_folder #Must be a subdirectory for download dependency reasons.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Images from Azure Blob Storage\n",
        "\n",
        "Set up parameters and download a sample of watermarked images to the local directory.\n",
        "Note that this only needs to be run once, and downloads a large number of files into your Azure AI file system (needing a lot of storage).\n",
        "To avoid accidental rerunning, we have set the variable \"DOWNLOAD_IMAGES\" to False. Switch it to True to run the cell.\n",
        "Similarly switch \"num_images\" to a larger number if you want to download the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from access_blob_storage_subfolder import download_blobs_from_subfolder\n",
        "\n",
        "\n",
        "## Step 1:\n",
        "# Download sample images to the 'embedding/inputfiles' folder\n",
        "if (DOWNLOAD_IMAGES == True and WatermarkMethod == \"Stable_Signature\"):\n",
        "    # Define parameters\n",
        "    account_name = \"\" # Azure Blob Storage account name\n",
        "    account_key = \"\"\n",
        "    container_name = \"embedding\" # Blob container name\n",
        "    download_path = \"./embedding_data/\" # Local download directory\n",
        "    # Define the sub-folder you want to access\n",
        "    sub_folder = \"raw_watermarked_images/\"  # Replace with your sub-folder name\n",
        "    num_images = 2   # Number of images to download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ACTION: Download the images.\n",
        "# Change the \"0 == 1\" to \"0 == 0\" on the following line to download images from Azure Blob Storage\n",
        "if (DOWNLOAD_IMAGES == True):\n",
        "    print(\"Downloading images from Azure Blob Storage...\")\n",
        "    download_blobs_from_subfolder(account_name, account_key, container_name, download_path, sub_folder, num_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Install required packages and import necessary libraries and custom modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install package needed to detect watermarks. Default environment is azureml_py38, scripts tends to be 3.10.14(\"watermark\" venv)\n",
        "import importlib.util\n",
        "if importlib.util.find_spec(\"timm\") is None:\n",
        "    !pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib.util\n",
        "if importlib.util.find_spec(\"trustmark\") is None:\n",
        "    !pip install trustmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Old CWD is: /mnt/batch/tasks/shared/LS_root/mounts/clusters/df-cpu/code/Users/David.Fletcher/ost-embedding-research/PotentialTransforms\n",
            "CWD set to: /mnt/batch/tasks/shared/LS_root/mounts/clusters/df-cpu/code/Users/David.Fletcher/ost-embedding-research/watermarkanything/watermark-anything\n"
          ]
        }
      ],
      "source": [
        "# If you are using the Watermark_Anything method, you will need to change the working directory to that repo.\n",
        "\n",
        "if (WatermarkMethod == \"Watermark_Anything\"):\n",
        "    import os\n",
        "\n",
        "    print(\"Old CWD is:\", os.getcwd())\n",
        "    # Set working directory to where the configs are expected\n",
        "    os.chdir(wk_dir_wa)\n",
        "    print(\"CWD set to:\", os.getcwd())\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "making attention of type 'vanilla' with 64 in_channels\n",
            "Working with z of shape (1, 68, 32, 32) = 69632 dimensions.\n",
            "making attention of type 'vanilla' with 64 in_channels\n",
            "Model loaded successfully from /home/azureuser/.cache/huggingface/hub/models--facebook--watermark-anything/snapshots/88ffb2cd1079a76b01d6beda8bf2a433c25fa881/checkpoint.pth\n",
            "{'embedder_config': 'configs/embedder.yaml', 'augmentation_config': 'configs/all_augs_multi_wm.yaml', 'extractor_config': 'configs/extractor.yaml', 'attenuation_config': 'configs/attenuation.yaml', 'embedder_model': 'vae_small', 'extractor_model': 'sam_base', 'nbits': 32, 'img_size': 256, 'img_size_extractor': 256, 'attenuation': 'jnd_1_3_blue', 'scaling_w': 2.0, 'scaling_w_schedule': None, 'scaling_i': 1.0, 'roll_probability': 0.2, 'multiple_w': 1.0, 'nb_wm_eval': 5, 'optimizer': 'AdamW,lr=1e-4', 'optimizer_d': None, 'scheduler': 'CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5', 'epochs': 200, 'batch_size': 8, 'batch_size_eval': 16, 'temperature': 1.0, 'workers': 8, 'to_freeze_embedder': None, 'lambda_w': 1.0, 'lambda_w2': 6.0, 'lambda_i': 0.0, 'lambda_d': 0.0, 'balanced': True, 'total_gnorm': 0.0, 'perceptual_loss': 'none', 'disc_start': 0, 'disc_num_layers': 2, 'only_eval': False, 'eval_freq': 5, 'saveimg_freq': 5, 'saveckpt_freq': 50, 'seed': 42, 'debug_slurm': False, 'local_rank': 0, 'master_port': 18482, 'is_slurm_job': True, 'n_nodes': 1, 'node_id': 0, 'global_rank': 0, 'world_size': 8, 'n_gpu_per_node': 8, 'is_master': True, 'multi_node': False, 'distributed': True}\n"
          ]
        }
      ],
      "source": [
        "## Load the Watermark-Anything model, to enable Watermark_Anything's detect_watermark function.\n",
        "\n",
        "if (WatermarkMethod == \"Watermark_Anything\"):\n",
        "    import os\n",
        "    import torch\n",
        "    from PIL import Image\n",
        "    from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "    import sys\n",
        "    sys.path.append(wk_dir_wa)\n",
        "\n",
        "    from watermark_anything.data.metrics import msg_predict_inference\n",
        "    from notebooks.inference_utils import (\n",
        "        load_model_from_checkpoint, \n",
        "        default_transform, \n",
        "        msg2str\n",
        "    )\n",
        "\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    ckpt_path = hf_hub_download(\n",
        "        repo_id=\"facebook/watermark-anything\",\n",
        "        filename=\"checkpoint.pth\"\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "    # Load the model\n",
        "    \n",
        "    exp_dir = wk_dir_wa_checkpoints\n",
        "    json_path = os.path.join(exp_dir, \"params.json\")\n",
        "\n",
        "    wam = load_model_from_checkpoint(json_path, ckpt_path).to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Now that the model is set up, define a detect_watermark function for Watermark-Anything.\n",
        "\n",
        "if (WatermarkMethod == \"Watermark_Anything\"):\n",
        "    \n",
        "    def detect_watermark(image_path, ckpt_path=None):\n",
        "        \"\"\"Detects and returns the watermark bits hidden in the image.\n",
        "            \n",
        "        Args:        image_path (str): Path to the watermarked image.\n",
        "                     ckpt_path (str, optional): Unused variable for compatibility.\n",
        "        \n",
        "        \"\"\"\n",
        "        # Load the watermarked image\n",
        "        img = Image.open(image_path).convert(\"RGB\")\n",
        "        img_tensor = default_transform(img).unsqueeze(0).to(device)\n",
        "        \n",
        "        # Detect watermark\n",
        "        preds = wam.detect(img_tensor)[\"preds\"]\n",
        "        mask_preds = torch.sigmoid(preds[:, 0, :, :])\n",
        "        bit_preds = preds[:, 1:, :, :]\n",
        "        \n",
        "        # Predict message\n",
        "        pred_message = msg_predict_inference(bit_preds, mask_preds).cpu().float()\n",
        "        confidence = torch.max(mask_preds).item()\n",
        "        \n",
        "        wm_file = \"File\"\n",
        "        # Print result\n",
        "        #print(f\"{wm_file}: {msg2str(pred_message[0])} (confidence: {confidence:.4f})\")\n",
        "        \n",
        "        return msg2str(pred_message[0])  \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "## If we are using the TrustMark method, set up this version of the detect_watermark function\n",
        "\n",
        "if (WatermarkMethod == \"TrustMark\"):\n",
        "    from trustmark import TrustMark\n",
        "    from PIL import Image\n",
        "\n",
        "    # Initialize TrustMark\n",
        "    tm = TrustMark(verbose=True, model_type='Q')\n",
        "    ckpt_path = 0 # Unused variable for compatibility, as TrustMark does not require a checkpoint path.\n",
        "\n",
        "    # --- Encoding Example ---\n",
        "    # Load an image\n",
        "\n",
        "    def detect_watermark(image_path, ckpt_path=None):\n",
        "        \"\"\"Detects and returns the watermark bits hidden in the image.\n",
        "            \n",
        "        Args:        image_path (str): Path to the watermarked image.\n",
        "                     ckpt_path (str, optional): Unused variable for compatibility.\n",
        "        \n",
        "        \"\"\"\n",
        "        # Load the watermarked image\n",
        "        watermarked_image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        # Decode the secret message\n",
        "        wm_secret, wm_present, wm_schema = tm.decode(watermarked_image)\n",
        "        \n",
        "        if wm_present:\n",
        "            decoded_str = wm_secret\n",
        "        else:\n",
        "            decoded_str = None\n",
        "        return decoded_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "## This is an example of using TrustMark's encoding and decoding functions.\n",
        "## The code to create the TrustMark-watermarked images is in the \"trustmark_test.py\" script,\n",
        "## at /ost-embedding-research/trustmark/trustmark_test.py\n",
        "\n",
        "if (TrustMark_Example_Run == True):\n",
        "    from trustmark import TrustMark\n",
        "    from PIL import Image\n",
        "\n",
        "    # Initialize TrustMark\n",
        "    tm = TrustMark(verbose=True, model_type='Q')\n",
        "\n",
        "    # --- Encoding Example ---\n",
        "    # Load an image\n",
        "    try:\n",
        "        sample_image_location = raw_images_dir + '/0_1ab8ea5ecaf859f061e099597d72b5ee_watermarked.png'\n",
        "        cover = Image.open(sample_image_location).convert('RGB')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: file not found. Please replace with the path to your image.\")\n",
        "        exit()\n",
        "        \n",
        "    # Encode a secret message and save the watermarked image\n",
        "    tm_save_location = trustmark_test_location + '/watermarked_image.png'\n",
        "    tm.encode(cover, 'eightlet').save(tm_save_location)\n",
        "    print(\"Image watermarked and saved as \", tm_save_location)\n",
        "\n",
        "\n",
        "    # --- Decoding Example ---\n",
        "    # Load the watermarked image\n",
        "    watermarked_image = Image.open(tm_save_location).convert('RGB')\n",
        "\n",
        "    # Decode the secret message\n",
        "    wm_secret, wm_present, wm_schema = tm.decode(watermarked_image)\n",
        "\n",
        "    if wm_present:\n",
        "        print(f'Extracted secret: {wm_secret}')\n",
        "    else:\n",
        "        print('No watermark detected.')\n",
        "\n",
        "    # --- Removal Example ---\n",
        "    # Remove the watermark and save the cleaned image\n",
        "    stego = Image.open('watermarked_image.png').convert('RGB')\n",
        "    im_recover = tm.remove_watermark(stego)\n",
        "    tm_recover_location = trustmark_test_location + '/recovered_image.png'\n",
        "    im_recover.save(tm_recover_location)\n",
        "    print(\"Watermark removed and saved as\", tm_recover_location)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "## If we are using the Stable_Signature method, set up this version of the detect_watermark function\n",
        "\n",
        "\n",
        "if (WatermarkMethod == \"Stable_Signature\"):\n",
        "    ## NOTE: We are defining the ckpt_path here, to be used in the functions later.\n",
        "    import torch\n",
        "    from torchvision import transforms\n",
        "    from PIL import Image\n",
        "    import argparse\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    from skimage.metrics import peak_signal_noise_ratio\n",
        "    import sys\n",
        "    import os\n",
        "\n",
        "    # Add the parent directory of attenuations.py and models.py to the Python path\n",
        "    detector_dir = home_dir + \"/Detector\"\n",
        "    sys.path.append(detector_dir)\n",
        "\n",
        "    from models import HiddenEncoder, HiddenDecoder, EncoderWithJND, EncoderDecoder\n",
        "    from attenuations import JND\n",
        "\n",
        "    # Define variable for location of the ckpt file\n",
        "    ckpt_path = detector_dir + \"/ckpt/hidden_replicate.pth\"\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Helper functions to convert between boolean message arrays and bit strings\n",
        "    def msg2str(msg):\n",
        "        return \"\".join(['1' if el else '0' for el in msg])\n",
        "\n",
        "    def str2msg(s):\n",
        "        return [True if el == '1' else False for el in s]\n",
        "\n",
        "    # Parameters class\n",
        "    class Params():\n",
        "        def __init__(self, encoder_depth: int, encoder_channels: int, decoder_depth: int, decoder_channels: int, num_bits: int,\n",
        "                    attenuation: str, scale_channels: bool, scaling_i: float, scaling_w: float):\n",
        "            # Encoder and decoder parameters\n",
        "            self.encoder_depth = encoder_depth\n",
        "            self.encoder_channels = encoder_channels\n",
        "            self.decoder_depth = decoder_depth\n",
        "            self.decoder_channels = decoder_channels\n",
        "            self.num_bits = num_bits\n",
        "            # Attenuation parameters\n",
        "            self.attenuation = attenuation\n",
        "            self.scale_channels = scale_channels\n",
        "            self.scaling_i = scaling_i\n",
        "            self.scaling_w = scaling_w\n",
        "\n",
        "    # Define image transforms (using ImageNet normalization)\n",
        "    NORMALIZE_IMAGENET = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                std=[0.229, 0.224, 0.225])\n",
        "    UNNORMALIZE_IMAGENET = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "                                                std=[1/0.229, 1/0.224, 1/0.225])\n",
        "    default_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        NORMALIZE_IMAGENET\n",
        "    ])\n",
        "\n",
        "    # Set up parameters\n",
        "    params = Params(\n",
        "        encoder_depth=4, encoder_channels=64, decoder_depth=8, decoder_channels=64, num_bits=48,\n",
        "        attenuation=\"jnd\", scale_channels=False, scaling_i=1, scaling_w=1.5\n",
        "    )\n",
        "\n",
        "    # Create encoder and decoder models\n",
        "    decoder = HiddenDecoder(\n",
        "        num_blocks=params.decoder_depth, \n",
        "        num_bits=params.num_bits, \n",
        "        channels=params.decoder_channels\n",
        "    )\n",
        "    encoder = HiddenEncoder(\n",
        "        num_blocks=params.encoder_depth, \n",
        "        num_bits=params.num_bits, \n",
        "        channels=params.encoder_channels\n",
        "    )\n",
        "    attenuation = JND(preprocess=UNNORMALIZE_IMAGENET) if params.attenuation == \"jnd\" else None\n",
        "    encoder_with_jnd = EncoderWithJND(\n",
        "        encoder, attenuation, params.scale_channels, params.scaling_i, params.scaling_w\n",
        "    )\n",
        "\n",
        "    # Move encoder and decoder to device\n",
        "    encoder_with_jnd = encoder_with_jnd.to(device).eval()\n",
        "    decoder = decoder.to(device).eval()\n",
        "\n",
        "    # Function to load a decoder from a checkpoint\n",
        "    def load_decoder(ckpt_path, decoder_depth, num_bits, decoder_channels, device):\n",
        "        \"\"\"Loads the HiddenDecoder model with weights from a checkpoint.\"\"\"\n",
        "        decoder_model = HiddenDecoder(num_blocks=decoder_depth, num_bits=num_bits, channels=decoder_channels)\n",
        "        state_dict = torch.load(ckpt_path, map_location=device)['encoder_decoder']\n",
        "        # Remove any \"module.\" prefixes if present\n",
        "        encoder_decoder_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "        decoder_state_dict = {k.replace('decoder.', ''): v \n",
        "                            for k, v in encoder_decoder_state_dict.items() if 'decoder' in k}\n",
        "        decoder_model.load_state_dict(decoder_state_dict)\n",
        "        decoder_model = decoder_model.to(device).eval()\n",
        "        return decoder_model\n",
        "\n",
        "    # Function to detect watermark from an image using the decoder model\n",
        "    def detect_watermark(image_path, ckpt_path, decoder_depth=8, num_bits=48, decoder_channels=64):\n",
        "        \"\"\"Detects and returns the watermark bits hidden in the image.\"\"\"\n",
        "        # Load watermarked image and resize to expected dimensions.\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img = img.resize((512, 512), Image.BICUBIC)\n",
        "        # Apply the default transform.\n",
        "        img_tensor = default_transform(img).unsqueeze(0).to(device)\n",
        "        # Load the decoder model.\n",
        "        decoder_model = load_decoder(ckpt_path, decoder_depth, num_bits, decoder_channels, device)\n",
        "        # Decode the watermark.\n",
        "        ft = decoder_model(img_tensor)\n",
        "        decoded_msg = ft > 0  # Threshold to obtain binary message\n",
        "        decoded_str = msg2str(decoded_msg.squeeze(0).cpu().numpy())\n",
        "        return decoded_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /mnt/batch/tasks/shared/LS_root/mounts/clusters/df-cpu/code/Users/David.Fletcher/ost-embedding-research/watermarkanything/watermark-anything\n",
            "Updated working directory: /mnt/batch/tasks/shared/LS_root/mounts/clusters/df-cpu/code/Users/David.Fletcher\n"
          ]
        }
      ],
      "source": [
        "## Set the current working directory back to the original location\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(wk_dir)  # Ensure the current working directory is in the path\n",
        "\n",
        "# Print the current working directory\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# Set the working directory to the notebook's directory (if needed)\n",
        "os.chdir(azure_root_dir + user_name)\n",
        "print(f\"Updated working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Image Transformations\n",
        "\n",
        "For each downloaded image, apply a suite of transformations (resize, flip, rotate, crop, blur, color jitter, etc.) and save the results to organized subfolders.\n",
        "These cells take a very long time to run. We suggest testing on a very small number of images at first (<10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/home/azureuser/cloudfiles/code/Users/David.Fletcher/ost-embedding-research//watermark-anything/outputs/watermarked/'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[179], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m image_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Loop through all images in the raw images folder\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_images_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     18\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(raw_images_path, image_name)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Check if the file is an image\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/azureuser/cloudfiles/code/Users/David.Fletcher/ost-embedding-research//watermark-anything/outputs/watermarked/'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "#from scratchfile import save_compressed_image\n",
        "import combined_transforms\n",
        "\n",
        "\n",
        "#### Step 2:  Apply the transforms to the images\n",
        "# For each image in the above folder, apply the transforms \n",
        "#  and save the images to sub directories of the local embedding_data folder.\n",
        "####\n",
        "\n",
        "# Counter for the number of images processed\n",
        "image_counter = 0\n",
        "\n",
        "# Loop through all images in the raw images folder\n",
        "for image_name in os.listdir(raw_images_path):\n",
        "    image_path = os.path.join(raw_images_path, image_name)\n",
        "    \n",
        "    # Check if the file is an image\n",
        "    if os.path.isfile(image_path) and image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        \n",
        "        # Perform transformations here (to be implemented)\n",
        "        print(f\"Processing image: {image_name}\")\n",
        "        if image_counter >= images_to_process:\n",
        "            print(f\"Processed {images_to_process} images, stopping further processing.\")\n",
        "            break  \n",
        "        image_counter = image_counter + 1\n",
        "\n",
        "        ## Compress image and save it to the output directory\n",
        "        #output_images_path = os.path.join(download_path, \"compressed_image/\", image_name)\n",
        "        #input_image = Image.open(image_path)\n",
        "        #save_compressed_image(input_image, output_images_path, quality=1)\n",
        "\n",
        "        ## Add other transformations from combined_transforms.py here\n",
        "        # Define output directories for each transformation\n",
        "        transform_output_dirs = {\n",
        "            \"resized\": os.path.join(download_path, \"resized_images/\"),\n",
        "            \"flipped\": os.path.join(download_path, \"flipped_images/\"),\n",
        "            \"rotated\": os.path.join(download_path, \"rotated_images/\"),\n",
        "            \"jittered\": os.path.join(download_path, \"jittered_images/\"),\n",
        "            \"normalized\": os.path.join(download_path, \"normalized_images/\"),\n",
        "            \"blurred\": os.path.join(download_path, \"blurred_images/\"),\n",
        "            \"cropped\": os.path.join(download_path, \"cropped_images/\"),\n",
        "            \"perspective\": os.path.join(download_path, \"perspective_images/\"),\n",
        "            \"erased\": os.path.join(download_path, \"erased_images/\"),\n",
        "            \"grayscale\": os.path.join(download_path, \"grayscale_images/\"),\n",
        "            \"text_overlay\": os.path.join(download_path, \"text_overlay_images/\"),\n",
        "            \"compressed\": os.path.join(download_path, \"compressed_images/\"),\n",
        "            \"brightness_adjusted\": os.path.join(download_path, \"brightness_adjusted_images/\"),\n",
        "            \"contrast_adjusted\": os.path.join(download_path, \"contrast_adjusted_images/\"),\n",
        "            \"saturation_adjusted\": os.path.join(download_path, \"saturation_adjusted_images/\"),\n",
        "            \"hue_adjusted\": os.path.join(download_path, \"hue_adjusted_images/\"),\n",
        "            \"gamma_adjusted\": os.path.join(download_path, \"gamma_adjusted_images/\"),\n",
        "            \"sharpness_adjusted\": os.path.join(download_path, \"sharpness_adjusted_images/\")\n",
        "        }\n",
        "\n",
        "        # Add bitmask directories for values between 0 and 8\n",
        "        for bitmask_value in range(9):\n",
        "            transform_output_dirs[f\"bitmask_{bitmask_value}\"] = os.path.join(download_path, f\"bitmask_{bitmask_value}_images/\")\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        for transform_dir in transform_output_dirs.values():\n",
        "            os.makedirs(transform_dir, exist_ok=True)\n",
        "\n",
        "        # Apply transformations and save images. Low priority TODO: Stop unneeded repetition. Breaking 'DRY' principle.\n",
        "        combined_transforms.resize_image(image_path, os.path.join(transform_output_dirs[\"resized\"], image_name))\n",
        "        combined_transforms.random_horizontal_flip(image_path, os.path.join(transform_output_dirs[\"flipped\"], image_name))\n",
        "        combined_transforms.fixed_rotation(image_path, os.path.join(transform_output_dirs[\"rotated\"], image_name))\n",
        "        combined_transforms.color_jitter(image_path, os.path.join(transform_output_dirs[\"jittered\"], image_name))\n",
        "        combined_transforms.normalize_image(image_path, os.path.join(transform_output_dirs[\"normalized\"], image_name))\n",
        "        combined_transforms.gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred\"], image_name))\n",
        "        combined_transforms.centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped\"], image_name))\n",
        "        combined_transforms.random_perspective(image_path, os.path.join(transform_output_dirs[\"perspective\"], image_name))\n",
        "        combined_transforms.random_erasing(image_path, os.path.join(transform_output_dirs[\"erased\"], image_name))\n",
        "        combined_transforms.grayscale(image_path, os.path.join(transform_output_dirs[\"grayscale\"], image_name))\n",
        "        combined_transforms.overlay_text(image_path, os.path.join(transform_output_dirs[\"text_overlay\"], image_name), text=\"Sample Text\")\n",
        "        combined_transforms.jpeg_compress(image_path, os.path.join(transform_output_dirs[\"compressed\"], image_name), quality=85)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness_adjusted\"], image_name), brightness_factor=1.2)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast_adjusted\"], image_name), contrast_factor=1.2)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation_adjusted\"], image_name), saturation_factor=1.2)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hue_adjusted\"], image_name), hue_factor=0.1)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma_adjusted\"], image_name), gamma=1.5)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness_adjusted\"], image_name), sharpness_factor=2.0)\n",
        "\n",
        "        # Apply bitmask transformations for values between 0 and 8\n",
        "        for bitmask_value in range(9):\n",
        "            combined_transforms.bitmask_image(image_path, os.path.join(transform_output_dirs[f\"bitmask_{bitmask_value}\"], image_name), bitmask_value)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "#from scratchfile import save_compressed_image\n",
        "import combined_transforms\n",
        "\n",
        "\n",
        "\n",
        "def scaled_centre_crop(image_path, output_path, scaling_factor=0.8):\n",
        "    \"\"\"\n",
        "    Wrapper function for centre_crop that calculates the size of the image,\n",
        "    scales it by a given factor, and crops the center of the image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "        output_path (str): Path to save the cropped image.\n",
        "        scaling_factor (float): Scaling factor between 0 and 1 to determine the crop size.\n",
        "                                Default is 0.8 (80% of the original size).\n",
        "    \"\"\"\n",
        "    # Ensure the scaling factor is valid\n",
        "    if not (0 < scaling_factor <= 1):\n",
        "        raise ValueError(\"Scaling factor must be between 0 and 1.\")\n",
        "\n",
        "    # Load the image to calculate its size\n",
        "    image = Image.open(image_path)\n",
        "    original_width, original_height = image.size\n",
        "\n",
        "    # Calculate the new crop size based on the scaling factor\n",
        "    new_width = int(original_width * scaling_factor)\n",
        "    new_height = int(original_height * scaling_factor)\n",
        "\n",
        "    # Call the centre_crop function with the calculated size\n",
        "    combined_transforms.centre_crop(image_path, output_path, size=(new_height, new_width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 2 images, stopping further processing.\n"
          ]
        }
      ],
      "source": [
        "##### Semi-scratch. Code to generate pictures with various percentage crops\n",
        "##### (99%, 90%, ... ..., 10%)\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "#from scratchfile import save_compressed_image\n",
        "import combined_transforms\n",
        "\n",
        "\n",
        "#### Step 2:  Apply the transforms to the images\n",
        "# For each image in the above folder, apply the transforms \n",
        "#  and save the images to sub directories of the local embedding_data folder.\n",
        "####\n",
        "\n",
        "# Counter for the number of images processed\n",
        "image_counter = 0\n",
        "\n",
        "# Loop through all images in the raw images folder\n",
        "for image_name in os.listdir(raw_images_path):\n",
        "    image_path = os.path.join(raw_images_path, image_name)\n",
        "    \n",
        "    # Check if the file is an image\n",
        "    if os.path.isfile(image_path) and image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        \n",
        "        # Perform transformations here (to be implemented)\n",
        "        if image_counter >= images_to_process:\n",
        "            print(f\"Processed {images_to_process} images, stopping further processing.\")\n",
        "            break  \n",
        "        image_counter = image_counter + 1\n",
        "\n",
        "        ## Compress image and save it to the output directory\n",
        "        #output_images_path = os.path.join(download_path, \"compressed_image/\", image_name)\n",
        "        #input_image = Image.open(image_path)\n",
        "        #save_compressed_image(input_image, output_images_path, quality=1)\n",
        "\n",
        "        ## Add other transformations from combined_transforms.py here\n",
        "        # Define output directories for each transformation\n",
        "        transform_output_dirs = {\n",
        "            \"cropped100\": os.path.join(download_path, \"cropped_images100/\"),\n",
        "            \"cropped99\": os.path.join(download_path, \"cropped_images99/\"),\n",
        "            \"cropped90\": os.path.join(download_path, \"cropped_images90/\"),\n",
        "            \"cropped80\": os.path.join(download_path, \"cropped_images80/\"),\n",
        "            \"cropped70\": os.path.join(download_path, \"cropped_images70/\"),\n",
        "            \"cropped60\": os.path.join(download_path, \"cropped_images60/\"),\n",
        "            \"cropped50\": os.path.join(download_path, \"cropped_images50/\"),\n",
        "            \"cropped40\": os.path.join(download_path, \"cropped_images40/\"),\n",
        "            \"cropped30\": os.path.join(download_path, \"cropped_images30/\"),\n",
        "            \"cropped20\": os.path.join(download_path, \"cropped_images20/\"),\n",
        "            \"cropped10\": os.path.join(download_path, \"cropped_images10/\"),\n",
        "        }\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        for transform_dir in transform_output_dirs.values():\n",
        "            os.makedirs(transform_dir, exist_ok=True)\n",
        "\n",
        "        # Apply transformations and save images. Low priority TODO: Stop unneeded repetition. Breaking 'DRY' principle.\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped100\"], image_name), scaling_factor=1)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped99\"], image_name), scaling_factor=0.99)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped90\"], image_name), scaling_factor=0.90)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped80\"], image_name), scaling_factor=0.80)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped70\"], image_name), scaling_factor=0.70)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped60\"], image_name), scaling_factor=0.60)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped50\"], image_name), scaling_factor=0.50)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped40\"], image_name), scaling_factor=0.40)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped30\"], image_name), scaling_factor=0.30)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped20\"], image_name), scaling_factor=0.20)\n",
        "        scaled_centre_crop(image_path, os.path.join(transform_output_dirs[\"cropped10\"], image_name), scaling_factor=0.10)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#####\n",
        "# ## Now we repeat that process for 'blur'. Ie the gaussian_blur function\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "#from scratchfile import save_compressed_image\n",
        "import combined_transforms\n",
        "\n",
        "\n",
        "def apply_gaussian_blur(image_path, output_path, kernel_size=51):\n",
        "    \"\"\"\n",
        "    Wrapper function for gaussian_blur that validates the kernel size\n",
        "    and applies Gaussian blur to the image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "        output_path (str): Path to save the blurred image.\n",
        "        kernel_size (int): Size of the Gaussian kernel. Must be an odd number.\n",
        "                           Default is 51.\n",
        "    \"\"\"\n",
        "    # Ensure the kernel size is a positive odd number\n",
        "    if kernel_size <= 0 or kernel_size % 2 == 0:\n",
        "        raise ValueError(\"Kernel size must be a positive odd number.\")\n",
        "\n",
        "    # Call the gaussian_blur function\n",
        "    combined_transforms.gaussian_blur(image_path, output_path, kernel_size=kernel_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing image: 0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "Processing image: 0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "Processing image: 0_1416aabf6d59fd4e348473adc87838f_original_trustmark_watermarked.png\n",
            "Processed 2 images, stopping further processing.\n"
          ]
        }
      ],
      "source": [
        "##### Semi-scratch. Code to generate pictures with various blurs\n",
        "##### (..., 51, ...)\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "#from scratchfile import save_compressed_image\n",
        "import combined_transforms\n",
        "\n",
        "\n",
        "#### Step 2:  Apply the transforms to the images\n",
        "# For each image in the above folder, apply the transforms \n",
        "#  and save the images to sub directories of the local embedding_data folder.\n",
        "####\n",
        "\n",
        "# Image counter (temporary variable)\n",
        "image_counter = 0\n",
        "\n",
        "# Loop through all images in the raw images folder\n",
        "for image_name in os.listdir(raw_images_path):\n",
        "    image_path = os.path.join(raw_images_path, image_name)\n",
        "    \n",
        "    # Check if the file is an image\n",
        "    if os.path.isfile(image_path) and image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        \n",
        "        # Perform transformations here (to be implemented)\n",
        "        print(f\"Processing image: {image_name}\")\n",
        "        if image_counter >= images_to_process:\n",
        "            print(f\"Processed {images_to_process} images, stopping further processing.\")\n",
        "            break  \n",
        "        image_counter = image_counter + 1\n",
        "\n",
        "\n",
        "        # Define output directories for each transformation\n",
        "        transform_output_dirs = {\n",
        "            \"blurred1\": os.path.join(download_path, \"blurred_images1/\"),\n",
        "            \"blurred3\": os.path.join(download_path, \"blurred_images3/\"),\n",
        "            \"blurred7\": os.path.join(download_path, \"blurred_images7/\"),\n",
        "            \"blurred15\": os.path.join(download_path, \"blurred_images15/\"),\n",
        "            \"blurred31\": os.path.join(download_path, \"blurred_images31/\"),\n",
        "            \"blurred51\": os.path.join(download_path, \"blurred_images51/\"),\n",
        "            \"blurred75\": os.path.join(download_path, \"blurred_images75/\"),\n",
        "            \"blurred101\": os.path.join(download_path, \"blurred_images101/\"),\n",
        "            \"blurred301\": os.path.join(download_path, \"blurred_images301/\"),\n",
        "            \"blurred501\": os.path.join(download_path, \"blurred_images501/\"),\n",
        "        }\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        for transform_dir in transform_output_dirs.values():\n",
        "            os.makedirs(transform_dir, exist_ok=True)\n",
        "\n",
        "        # Apply transformations and save images. Low priority TODO: Stop unneeded repetition. Breaking 'DRY' principle.\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred1\"], image_name), kernel_size=1)\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred3\"], image_name), kernel_size=3)\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred7\"], image_name), kernel_size=7)\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred15\"], image_name), kernel_size=15)\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred31\"], image_name), kernel_size=31)\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred51\"], image_name), kernel_size=51)\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred75\"], image_name), kernel_size=75)\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred101\"], image_name), kernel_size=101)\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred301\"], image_name), kernel_size=301)\n",
        "        apply_gaussian_blur(image_path, os.path.join(transform_output_dirs[\"blurred501\"], image_name), kernel_size=501)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "#from scratchfile import save_compressed_image\n",
        "import combined_transforms\n",
        "\n",
        "\n",
        "\n",
        "# Define the path to the raw images folder\n",
        "row_counter = 0\n",
        "# Loop through all images in the raw images folder\n",
        "for image_name in os.listdir(raw_images_path):\n",
        "    image_path = os.path.join(raw_images_path, image_name)\n",
        "    row_counter += 1\n",
        "    if row_counter  == images_to_process:\n",
        "        break\n",
        "    # Check if the file is an image\n",
        "    if os.path.isfile(image_path) and image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        \n",
        "        # Perform transformations here (to be implemented)\n",
        "\n",
        "\n",
        "        # Define output directories for each transformation. Done via tab completion.\n",
        "        transform_output_dirs = {\n",
        "            \"brightness0.25\": os.path.join(download_path, \"brightened_images0-25/\"),\n",
        "            \"brightness0.5\": os.path.join(download_path, \"brightened_images0-5/\"),\n",
        "            \"brightness0.7\": os.path.join(download_path, \"brightened_images0-7/\"),\n",
        "            \"brightness0.8\": os.path.join(download_path, \"brightened_images0-8/\"),\n",
        "            \"brightness0.9\": os.path.join(download_path, \"brightened_images0-9/\"),\n",
        "            \"brightness1\": os.path.join(download_path, \"brightened_images1/\"),\n",
        "            \"brightness1.1\": os.path.join(download_path, \"brightened_images1-1/\"),\n",
        "            \"brightness1.2\": os.path.join(download_path, \"brightened_images1-2/\"),\n",
        "            \"brightness1.3\": os.path.join(download_path, \"brightened_images1-3/\"),\n",
        "            \"brightness1.5\": os.path.join(download_path, \"brightened_images1-5/\"),\n",
        "            \"brightness1.75\": os.path.join(download_path, \"brightened_images1-75/\"),\n",
        "            \"contrast0.25\": os.path.join(download_path, \"contrast_images0-25/\"),\n",
        "            \"contrast0.5\": os.path.join(download_path, \"contrast_images0-5/\"),\n",
        "            \"contrast0.7\": os.path.join(download_path, \"contrast_images0-7/\"),\n",
        "            \"contrast0.8\": os.path.join(download_path, \"contrast_images0-8/\"),\n",
        "            \"contrast0.9\": os.path.join(download_path, \"contrast_images0-9/\"),\n",
        "            \"contrast1\": os.path.join(download_path, \"contrast_images1/\"),\n",
        "            \"contrast1.1\": os.path.join(download_path, \"contrast_images1-1/\"),\n",
        "            \"contrast1.2\": os.path.join(download_path, \"contrast_images1-2/\"),\n",
        "            \"contrast1.3\": os.path.join(download_path, \"contrast_images1-3/\"),\n",
        "            \"contrast1.5\": os.path.join(download_path, \"contrast_images1-5/\"),\n",
        "            \"contrast1.75\": os.path.join(download_path, \"contrast_images1-75/\"),\n",
        "            \"gamma0.25\": os.path.join(download_path, \"gamma_images0-25/\"),\n",
        "            \"gamma0.5\": os.path.join(download_path, \"gamma_images0-5/\"),\n",
        "            \"gamma0.7\": os.path.join(download_path, \"gamma_images0-7/\"),\n",
        "            \"gamma0.8\": os.path.join(download_path, \"gamma_images0-8/\"),\n",
        "            \"gamma0.9\": os.path.join(download_path, \"gamma_images0-9/\"),\n",
        "            \"gamma1\": os.path.join(download_path, \"gamma_images1/\"),\n",
        "            \"gamma1.1\": os.path.join(download_path, \"gamma_images1-1/\"),\n",
        "            \"gamma1.2\": os.path.join(download_path, \"gamma_images1-2/\"),\n",
        "            \"gamma1.3\": os.path.join(download_path, \"gamma_images1-3/\"),\n",
        "            \"gamma1.5\": os.path.join(download_path, \"gamma_images1-5/\"),\n",
        "            \"gamma1.75\": os.path.join(download_path, \"gamma_images1-75/\"),\n",
        "            \"hueminus0.5\": os.path.join(download_path, \"hue_imagesminus0-5/\"),\n",
        "            \"hueminus0.3\": os.path.join(download_path, \"hue_imagesminus0-3/\"),\n",
        "            \"hueminus0.2\": os.path.join(download_path, \"hue_imagesminus0-2/\"),\n",
        "            \"hueminus0.1\": os.path.join(download_path, \"hue_imagesminus0-1/\"),\n",
        "            \"hue0\": os.path.join(download_path, \"hue_images0/\"),\n",
        "            \"hue0.1\": os.path.join(download_path, \"hue_images0-1/\"),\n",
        "            \"hue0.2\": os.path.join(download_path, \"hue_images0-2/\"),\n",
        "            \"hue0.3\": os.path.join(download_path, \"hue_images0-3/\"),\n",
        "            \"hue0.5\": os.path.join(download_path, \"hue_images0-5/\"),\n",
        "            \"saturation0.25\": os.path.join(download_path, \"saturation_images0-25/\"),\n",
        "            \"saturation0.5\": os.path.join(download_path, \"saturation_images0-5/\"),\n",
        "            \"saturation0.7\": os.path.join(download_path, \"saturation_images0-7/\"),\n",
        "            \"saturation0.8\": os.path.join(download_path, \"saturation_images0-8/\"),\n",
        "            \"saturation0.9\": os.path.join(download_path, \"saturation_images0-9/\"),\n",
        "            \"saturation1\": os.path.join(download_path, \"saturation_images1/\"),\n",
        "            \"saturation1.1\": os.path.join(download_path, \"saturation_images1-1/\"),\n",
        "            \"saturation1.2\": os.path.join(download_path, \"saturation_images1-2/\"),\n",
        "            \"saturation1.3\": os.path.join(download_path, \"saturation_images1-3/\"),\n",
        "            \"saturation1.5\": os.path.join(download_path, \"saturation_images1-5/\"),\n",
        "            \"saturation1.75\": os.path.join(download_path, \"saturation_images1-75/\"),\n",
        "            \"sharpness0.25\": os.path.join(download_path, \"sharpness_images0-25/\"),\n",
        "            \"sharpness0.5\": os.path.join(download_path, \"sharpness_images0-5/\"),\n",
        "            \"sharpness0.7\": os.path.join(download_path, \"sharpness_images0-7/\"),\n",
        "            \"sharpness0.8\": os.path.join(download_path, \"sharpness_images0-8/\"),\n",
        "            \"sharpness0.9\": os.path.join(download_path, \"sharpness_images0-9/\"),\n",
        "            \"sharpness1\": os.path.join(download_path, \"sharpness_images1/\"),\n",
        "            \"sharpness1.1\": os.path.join(download_path, \"sharpness_images1-1/\"),\n",
        "            \"sharpness1.2\": os.path.join(download_path, \"sharpness_images1-2/\"),\n",
        "            \"sharpness1.3\": os.path.join(download_path, \"sharpness_images1-3/\"),\n",
        "            \"sharpness1.5\": os.path.join(download_path, \"sharpness_images1-5/\"),\n",
        "            \"sharpness1.75\": os.path.join(download_path, \"sharpness_images1-75/\"),\n",
        "        }\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        for transform_dir in transform_output_dirs.values():\n",
        "            os.makedirs(transform_dir, exist_ok=True)\n",
        "\n",
        "        # Apply transformations and save images. Low priority TODO: Stop unneeded repetition. Breaking 'DRY' principle.\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness0.25\"], image_name), brightness_factor=0.25)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness0.5\"], image_name), brightness_factor=0.5)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness0.7\"], image_name), brightness_factor=0.7)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness0.8\"], image_name), brightness_factor=0.8)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness0.9\"], image_name), brightness_factor=0.9)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness1\"], image_name), brightness_factor=1)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness1.1\"], image_name), brightness_factor=1.1)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness1.2\"], image_name), brightness_factor=1.2)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness1.3\"], image_name), brightness_factor=1.3)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness1.5\"], image_name), brightness_factor=1.5)\n",
        "        combined_transforms.adjust_brightness(image_path, os.path.join(transform_output_dirs[\"brightness1.75\"], image_name), brightness_factor=1.75)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast0.25\"], image_name), contrast_factor=0.25)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast0.5\"], image_name), contrast_factor=0.5)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast0.7\"], image_name), contrast_factor=0.7)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast0.8\"], image_name), contrast_factor=0.8)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast0.9\"], image_name), contrast_factor=0.9)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast1\"], image_name), contrast_factor=1)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast1.1\"], image_name), contrast_factor=1.1)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast1.2\"], image_name), contrast_factor=1.2)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast1.3\"], image_name), contrast_factor=1.3)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast1.5\"], image_name), contrast_factor=1.5)\n",
        "        combined_transforms.adjust_contrast(image_path, os.path.join(transform_output_dirs[\"contrast1.75\"], image_name), contrast_factor=1.75)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma0.25\"], image_name), gamma=0.25)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma0.5\"], image_name), gamma=0.5)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma0.7\"], image_name), gamma=0.7)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma0.8\"], image_name), gamma=0.8)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma0.9\"], image_name), gamma=0.9)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma1\"], image_name), gamma=1)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma1.1\"], image_name), gamma=1.1)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma1.2\"], image_name), gamma=1.2)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma1.3\"], image_name), gamma=1.3)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma1.5\"], image_name), gamma=1.5)\n",
        "        combined_transforms.adjust_gamma(image_path, os.path.join(transform_output_dirs[\"gamma1.75\"], image_name), gamma=1.75)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hueminus0.5\"], image_name), hue_factor=-0.5)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hueminus0.3\"], image_name), hue_factor=-0.3)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hueminus0.2\"], image_name), hue_factor=-0.2)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hueminus0.1\"], image_name), hue_factor=-0.1)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hue0\"], image_name), hue_factor=0)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hue0.1\"], image_name), hue_factor=0.1)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hue0.2\"], image_name), hue_factor=0.2)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hue0.3\"], image_name), hue_factor=0.3)\n",
        "        combined_transforms.adjust_hue(image_path, os.path.join(transform_output_dirs[\"hue0.5\"], image_name), hue_factor=0.5)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation0.25\"], image_name), saturation_factor=0.25)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation0.5\"], image_name), saturation_factor=0.5)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation0.7\"], image_name), saturation_factor=0.7)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation0.8\"], image_name), saturation_factor=0.8)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation0.9\"], image_name), saturation_factor=0.9)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation1\"], image_name), saturation_factor=1)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation1.1\"], image_name), saturation_factor=1.1)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation1.2\"], image_name), saturation_factor=1.2)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation1.3\"], image_name), saturation_factor=1.3)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation1.5\"], image_name), saturation_factor=1.5)\n",
        "        combined_transforms.adjust_saturation(image_path, os.path.join(transform_output_dirs[\"saturation1.75\"], image_name), saturation_factor=1.75)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness0.25\"], image_name), sharpness_factor=0.25)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness0.5\"], image_name), sharpness_factor=0.5)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness0.7\"], image_name), sharpness_factor=0.7)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness0.8\"], image_name), sharpness_factor=0.8)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness0.9\"], image_name), sharpness_factor=0.9)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness1\"], image_name), sharpness_factor=1)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness1.1\"], image_name), sharpness_factor=1.1)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness1.2\"], image_name), sharpness_factor=1.2)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness1.3\"], image_name), sharpness_factor=1.3)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness1.5\"], image_name), sharpness_factor=1.5)\n",
        "        combined_transforms.adjust_sharpness(image_path, os.path.join(transform_output_dirs[\"sharpness1.75\"], image_name), sharpness_factor=1.75)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reporting and Analysis\n",
        "\n",
        "Outline of next steps:\n",
        "- Calculate scores for each image pair\n",
        "- Test watermark presence in each transformed image\n",
        "- Collect and save results to CSV\n",
        "- (Optional) Upload results to blob storage\n",
        "- Generate reports and visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Metrics and Detect Watermarks\n",
        "\n",
        "For each transformed image, compare it to the original using various metrics (PSNR, SSIM, LBP, GLCM, etc.), detect watermarks, and save the results to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 2 images, stopping further processing.\n"
          ]
        }
      ],
      "source": [
        "# Imported CoPilot code. Needs bug testing and clarification.\n",
        "\n",
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from metrics import calculate_texture_features, calculate_similarity_metrics ,calculate_image_metrics\n",
        "import cv2\n",
        "\n",
        "import importlib.util\n",
        "\n",
        "##Below code paragraph imports compare_images.py from the specified path\n",
        "module_path = home_dir + '/utils/compare_images.py'\n",
        "spec = importlib.util.spec_from_file_location(\"compare_images\", module_path)\n",
        "compare_images = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(compare_images)\n",
        "\n",
        "\n",
        "image_counter = 0 \n",
        "\n",
        "# Define the paths\n",
        "## TODO: Issue where StableSig was running with \"raw_images_path = os.path.join(download_path, raw_watermarked_images) here\n",
        "##       Unsure of the implications there.\n",
        "\n",
        "transformed_folders = [\n",
        "                        \"bitmask_0_images\", #Could be simplified by looping through all folders instead of hardcoding\n",
        "                        \"bitmask_1_images\", #However that could cause issues if extra folders are accidentally added.\n",
        "                        \"bitmask_2_images\", #That is an option for the final version\n",
        "                        \"bitmask_3_images\",\n",
        "                        \"bitmask_4_images\",\n",
        "                        \"bitmask_5_images\",\n",
        "                        \"bitmask_6_images\",\n",
        "                        \"bitmask_7_images\",\n",
        "                        \"bitmask_8_images\",\n",
        "                        \"blurred_images\",\n",
        "                        \"brightness_adjusted_images\",\n",
        "                        \"compressed_images\",\n",
        "                        \"contrast_adjusted_images\",\n",
        "                        \"cropped_images\",\n",
        "                        \"erased_images\",\n",
        "                        \"flipped_images\",\n",
        "                        \"gamma_adjusted_images\",\n",
        "                        \"grayscale_images\",\n",
        "                        \"hue_adjusted_images\",\n",
        "                        \"jittered_images\",\n",
        "                        \"normalized_images\",\n",
        "                        \"perspective_images\",\n",
        "                        \"resized_images\",\n",
        "                        \"rotated_images\",\n",
        "                        \"saturation_adjusted_images\",\n",
        "                        \"sharpness_adjusted_images\",\n",
        "                        \"text_overlay_images\",\n",
        "                        ]    \n",
        "\n",
        "transformed_folders_paths = [os.path.join(download_path, folder) for folder in transformed_folders]\n",
        "\n",
        "# Define the output CSV file\n",
        "output_csv_path = os.path.join(download_path, \"comparison_results.csv\")\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open(output_csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)\n",
        "    \n",
        "    # Write the header row\n",
        "    header = [\"Image Name\", \"Transform\", \"PSNR\", \"SSIM_value\", \n",
        "                \"LBP_sim\", \"glcm_sim_contrast\", \"glcm_sim_dissimilarity\",\n",
        "                \"glcm_sim_homogeneity\", \"glcm_sim_energy\",\n",
        "                \"glcm_sim_correlation\", \"psnr2\", \"ssim_value2\",\n",
        "                \"Watermark_raw\", \"Watermark_transformed\", \"Watermark_present\"]  # Add more columns for additional placeholder functions\n",
        "    csv_writer.writerow(header)\n",
        "    \n",
        "    row_counter = 1\n",
        "    image_counter = 0\n",
        "    # Loop through all images in the raw images folder\n",
        "    for image_name in os.listdir(raw_images_path):\n",
        "        temp_image_path = os.path.join(raw_images_path, image_name)\n",
        "        if row_counter % 10 == 0:\n",
        "            print(f\"Processing image {row_counter}\")\n",
        "        row_counter += 1\n",
        "        if image_counter >= images_to_process:\n",
        "            print(f\"Processed {images_to_process} images, stopping further processing.\")\n",
        "            break  \n",
        "        image_counter = image_counter + 1\n",
        "        \n",
        "        # Check if the file is an image\n",
        "        if os.path.isfile(temp_image_path) and image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            # Open the raw image\n",
        "            with Image.open(temp_image_path) as raw_image:\n",
        "                # Loop through each transformed folder\n",
        "                for folder, folder_path in zip(transformed_folders, transformed_folders_paths):\n",
        "                    transformed_image_path = os.path.join(folder_path, image_name)\n",
        "                    \n",
        "                    # Check if the transformed image exists\n",
        "                    if os.path.isfile(transformed_image_path):\n",
        "                        with Image.open(transformed_image_path) as transformed_image:\n",
        "                            \n",
        "                            \n",
        "                            # Detect watermark in raw image\n",
        "                            detected_bits_raw = detect_watermark(temp_image_path, ckpt_path)\n",
        "\n",
        "                            # Detect watermark in transformed image\n",
        "                            detected_bits_transformed = detect_watermark(transformed_image_path, ckpt_path)\n",
        "\n",
        "                            if (detected_bits_raw == detected_bits_transformed):\n",
        "                                watermark_present = 1\n",
        "                            else:\n",
        "                                watermark_present = 0\n",
        "\n",
        "                            # Check if the images are the same size\n",
        "                            if raw_image.size != transformed_image.size:\n",
        "                                psnr = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                ssim_value = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                LBP_sim = \"NA\"\n",
        "                                glcm_sim_contrast = \"NA\"\n",
        "                                glcm_sim_dissimilarity = \"NA\"\n",
        "                                glcm_sim_homogeneity = \"NA\"\n",
        "                                glcm_sim_energy = \"NA\"\n",
        "                                glcm_sim_correlation = \"NA\"\n",
        "                                psnr2 = \"NA\"\n",
        "                                ssim_value2 = \"NA\"\n",
        "\n",
        "                            else:\n",
        "                                # Convert PIL images to NumPy arrays\n",
        "                                raw_image_array = np.array(raw_image)\n",
        "                                transformed_image_array = np.array(transformed_image)\n",
        "                                \n",
        "                                # Apply the comparison function\n",
        "                                psnr, ssim_value = compare_images.calculate_image_metrics(raw_image_array, transformed_image_array)\n",
        "                                psnr = float(psnr)\n",
        "                                ssim_value = float(ssim_value)\n",
        "\n",
        "                                # Calculate metrics from metrics.py\n",
        "                                texture_metrics_original = calculate_texture_features(image = cv2.imread(temp_image_path))\n",
        "                                texture_metrics_transformed = calculate_texture_features(image = cv2.imread(transformed_image_path))\n",
        "                                similarity_metrics = calculate_similarity_metrics(texture_metrics_original, texture_metrics_transformed)\n",
        "                                image_metrics = calculate_image_metrics(cv2.imread(temp_image_path), cv2.imread(transformed_image_path))\n",
        "\n",
        "                                LBP_sim = similarity_metrics['lbp_similarity']\n",
        "                                glcm_sim_contrast = similarity_metrics['glcm_similarities']['contrast']\n",
        "                                glcm_sim_dissimilarity = similarity_metrics['glcm_similarities']['dissimilarity']\n",
        "                                glcm_sim_homogeneity = similarity_metrics['glcm_similarities']['homogeneity']\n",
        "                                glcm_sim_energy = similarity_metrics['glcm_similarities']['energy']\n",
        "                                glcm_sim_correlation = similarity_metrics['glcm_similarities']['correlation']\n",
        "                                psnr2 = image_metrics[0]\n",
        "                                ssim_value2 = image_metrics[1]\n",
        "\n",
        "\n",
        "                            # Write the result to the CSV file\n",
        "                            csv_writer.writerow([image_name, folder, psnr, ssim_value, \n",
        "                                                LBP_sim, glcm_sim_contrast, glcm_sim_dissimilarity,\n",
        "                                                glcm_sim_homogeneity, glcm_sim_energy,\n",
        "                                                glcm_sim_correlation, psnr2, ssim_value2,\n",
        "                                                detected_bits_raw, detected_bits_transformed, watermark_present])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 2 images, stopping further processing.\n"
          ]
        }
      ],
      "source": [
        "# Imported CoPilot code. Needs bug testing and clarification.\n",
        "\n",
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from metrics import calculate_texture_features, calculate_similarity_metrics ,calculate_image_metrics\n",
        "import cv2\n",
        "\n",
        "import importlib.util\n",
        "\n",
        "##Below code paragraph imports compare_images.py from the specified path\n",
        "module_path = home_dir + '/utils/compare_images.py'\n",
        "spec = importlib.util.spec_from_file_location(\"compare_images\", module_path)\n",
        "compare_images = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(compare_images)\n",
        "\n",
        "# Define a placeholder function\n",
        "def compare(image1, image2):\n",
        "    \"\"\"Placeholder function to compare two images and return a number.\"\"\"\n",
        "    # Replace this with your actual comparison logic\n",
        "    return 0.0\n",
        "\n",
        "# Define the paths\n",
        "transformed_folders = [\n",
        "                        \"cropped_images100\",\n",
        "                        \"cropped_images99\",\n",
        "                        \"cropped_images90\",\n",
        "                        \"cropped_images80\",\n",
        "                        \"cropped_images70\",\n",
        "                        \"cropped_images60\",\n",
        "                        \"cropped_images50\",\n",
        "                        \"cropped_images40\",\n",
        "                        \"cropped_images30\",\n",
        "                        \"cropped_images20\",\n",
        "                        \"cropped_images10\",\n",
        "                        ]    \n",
        "\n",
        "transformed_folders_paths = [os.path.join(download_path, folder) for folder in transformed_folders]\n",
        "\n",
        "# Define the output CSV file\n",
        "output_csv_path = os.path.join(download_path, \"comparison_results_cropped_deepdive.csv\")\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open(output_csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)\n",
        "    \n",
        "    # Write the header row\n",
        "    header = [\"Image Name\", \"Transform\", \"PSNR\", \"SSIM_value\", \n",
        "                \"LBP_sim\", \"glcm_sim_contrast\", \"glcm_sim_dissimilarity\",\n",
        "                \"glcm_sim_homogeneity\", \"glcm_sim_energy\",\n",
        "                \"glcm_sim_correlation\", \"psnr2\", \"ssim_value2\",\n",
        "                \"Watermark_raw\", \"Watermark_transformed\", \"Watermark_present\"]  # Add more columns for additional placeholder functions\n",
        "    csv_writer.writerow(header)\n",
        "    \n",
        "    row_counter = 1\n",
        "    image_counter = 0\n",
        "    # Loop through all images in the raw images folder\n",
        "    for image_name in os.listdir(raw_images_path):\n",
        "        temp_image_path = os.path.join(raw_images_path, image_name)\n",
        "        if row_counter % 10 == 0:\n",
        "            print(f\"Processing image {row_counter}\")\n",
        "        row_counter += 1\n",
        "        if image_counter >= images_to_process + 2:\n",
        "            print(f\"Processed {images_to_process} images, stopping further processing.\")\n",
        "            break  \n",
        "        image_counter = image_counter + 1\n",
        "        \n",
        "        # Check if the file is an image\n",
        "        if os.path.isfile(temp_image_path) and image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            # Open the raw image\n",
        "            with Image.open(temp_image_path) as raw_image:\n",
        "                # Loop through each transformed folder\n",
        "                for folder, folder_path in zip(transformed_folders, transformed_folders_paths):\n",
        "                    transformed_image_path = os.path.join(folder_path, image_name)\n",
        "                    \n",
        "                    # Check if the transformed image exists\n",
        "                    if os.path.isfile(transformed_image_path):\n",
        "                        with Image.open(transformed_image_path) as transformed_image:\n",
        "                            \n",
        "                            # Detect if there is a watermark in the raw image\n",
        "                            detected_bits_raw = detect_watermark(temp_image_path, ckpt_path)\n",
        "\n",
        "                            # Detect if there is a watermark in the transformed image\n",
        "                            detected_bits_transformed = detect_watermark(transformed_image_path, ckpt_path)\n",
        "\n",
        "                            if (detected_bits_raw == detected_bits_transformed):\n",
        "                                watermark_present = 1\n",
        "                            else:\n",
        "                                watermark_present = 0\n",
        "\n",
        "                            # Check if the images are the same size\n",
        "                            if raw_image.size != transformed_image.size:\n",
        "                                psnr = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                ssim_value = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                LBP_sim = \"NA\"\n",
        "                                glcm_sim_contrast = \"NA\"\n",
        "                                glcm_sim_dissimilarity = \"NA\"\n",
        "                                glcm_sim_homogeneity = \"NA\"\n",
        "                                glcm_sim_energy = \"NA\"\n",
        "                                glcm_sim_correlation = \"NA\"\n",
        "                                psnr2 = \"NA\"\n",
        "                                ssim_value2 = \"NA\"\n",
        "\n",
        "                            else:\n",
        "                                # Convert PIL images to NumPy arrays\n",
        "                                raw_image_array = np.array(raw_image)\n",
        "                                transformed_image_array = np.array(transformed_image)\n",
        "                                \n",
        "                                # Apply the comparison function\n",
        "                                psnr, ssim_value = compare_images.calculate_image_metrics(raw_image_array, transformed_image_array)\n",
        "                                psnr = float(psnr)\n",
        "                                ssim_value = float(ssim_value)\n",
        "\n",
        "                                # Calculate metrics from metrics.py\n",
        "                                texture_metrics_original = calculate_texture_features(image = cv2.imread(temp_image_path))\n",
        "                                texture_metrics_transformed = calculate_texture_features(image = cv2.imread(transformed_image_path))\n",
        "                                similarity_metrics = calculate_similarity_metrics(texture_metrics_original, texture_metrics_transformed)\n",
        "                                image_metrics = calculate_image_metrics(cv2.imread(temp_image_path), cv2.imread(transformed_image_path))\n",
        "\n",
        "                                LBP_sim = similarity_metrics['lbp_similarity']\n",
        "                                glcm_sim_contrast = similarity_metrics['glcm_similarities']['contrast']\n",
        "                                glcm_sim_dissimilarity = similarity_metrics['glcm_similarities']['dissimilarity']\n",
        "                                glcm_sim_homogeneity = similarity_metrics['glcm_similarities']['homogeneity']\n",
        "                                glcm_sim_energy = similarity_metrics['glcm_similarities']['energy']\n",
        "                                glcm_sim_correlation = similarity_metrics['glcm_similarities']['correlation']\n",
        "                                psnr2 = image_metrics[0]\n",
        "                                ssim_value2 = image_metrics[1]\n",
        "\n",
        "\n",
        "                            # Write the result to the CSV file\n",
        "                            csv_writer.writerow([image_name, folder, psnr, ssim_value, \n",
        "                                                LBP_sim, glcm_sim_contrast, glcm_sim_dissimilarity,\n",
        "                                                glcm_sim_homogeneity, glcm_sim_energy,\n",
        "                                                glcm_sim_correlation, psnr2, ssim_value2,\n",
        "                                                detected_bits_raw, detected_bits_transformed, watermark_present])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 2 images, stopping further processing.\n"
          ]
        }
      ],
      "source": [
        "# Imported CoPilot code. Needs bug testing and clarification.\n",
        "\n",
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from metrics import calculate_texture_features, calculate_similarity_metrics ,calculate_image_metrics\n",
        "import cv2\n",
        "\n",
        "import importlib.util\n",
        "\n",
        "##Below code paragraph imports compare_images.py from the specified path\n",
        "module_path = home_dir + '/utils/compare_images.py'\n",
        "spec = importlib.util.spec_from_file_location(\"compare_images\", module_path)\n",
        "compare_images = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(compare_images)\n",
        "\n",
        "# Define the paths\n",
        "transformed_folders = [\n",
        "                        \"blurred_images1\",\n",
        "                        \"blurred_images3\",\n",
        "                        \"blurred_images7\",\n",
        "                        \"blurred_images15\",\n",
        "                        \"blurred_images31\",\n",
        "                        \"blurred_images51\",\n",
        "                        \"blurred_images75\",\n",
        "                        \"blurred_images101\",\n",
        "                        \"blurred_images301\",\n",
        "                        \"blurred_images501\",\n",
        "                        ]    \n",
        "\n",
        "transformed_folders_paths = [os.path.join(download_path, folder) for folder in transformed_folders]\n",
        "\n",
        "# Define the output CSV file\n",
        "output_csv_path = os.path.join(download_path, \"comparison_results_blurred_deepdivefullsuite.csv\")\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open(output_csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)\n",
        "    \n",
        "    # Write the header row\n",
        "    header = [\"Image Name\", \"Transform\", \"PSNR\", \"SSIM_value\", \n",
        "                \"LBP_sim\", \"glcm_sim_contrast\", \"glcm_sim_dissimilarity\",\n",
        "                \"glcm_sim_homogeneity\", \"glcm_sim_energy\",\n",
        "                \"glcm_sim_correlation\", \"psnr2\", \"ssim_value2\",\n",
        "                \"Watermark_raw\", \"Watermark_transformed\", \"Watermark_present\"]  # Add more columns for additional placeholder functions\n",
        "    csv_writer.writerow(header)\n",
        "    \n",
        "    row_counter = 1\n",
        "    image_counter = 0\n",
        "    # Loop through all images in the raw images folder\n",
        "    for image_name in os.listdir(raw_images_path):\n",
        "        temp_image_path = os.path.join(raw_images_path, image_name)\n",
        "        if row_counter % 10 == 0:\n",
        "            print(f\"Processing image {row_counter}\")\n",
        "        row_counter += 1\n",
        "        if image_counter >= images_to_process + 2:\n",
        "            print(f\"Processed {images_to_process} images, stopping further processing.\")\n",
        "            break  \n",
        "        image_counter = image_counter + 1\n",
        "        \n",
        "        #print(\"a\", row_counter)\n",
        "        # Check if the file is an image\n",
        "        if os.path.isfile(temp_image_path) and image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            # Open the raw image\n",
        "            #print(\"b\", row_counter)\n",
        "            with Image.open(temp_image_path) as raw_image:\n",
        "                # Loop through each transformed folder\n",
        "                for folder, folder_path in zip(transformed_folders, transformed_folders_paths):\n",
        "                    transformed_image_path = os.path.join(folder_path, image_name)\n",
        "                    #print(\"in transformed loop\", row_counter)\n",
        "                    # Check if the transformed image exists\n",
        "                    if os.path.isfile(transformed_image_path):\n",
        "                        with Image.open(transformed_image_path) as transformed_image:\n",
        "                            #print(\"in transformed image\", row_counter)\n",
        "                            # Detect if there is a watermark in the raw image\n",
        "                            detected_bits_raw = detect_watermark(temp_image_path, ckpt_path)\n",
        "\n",
        "                            # Detect if there is a watermark in the transformed image\n",
        "                            detected_bits_transformed = detect_watermark(transformed_image_path, ckpt_path)\n",
        "\n",
        "                            if (detected_bits_raw == detected_bits_transformed):\n",
        "                                watermark_present = 1\n",
        "                            else:\n",
        "                                watermark_present = 0\n",
        "\n",
        "                            # Check if the images are the same size\n",
        "                            if raw_image.size != transformed_image.size:\n",
        "                                psnr = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                ssim_value = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                LBP_sim = \"NA\"\n",
        "                                glcm_sim_contrast = \"NA\"\n",
        "                                glcm_sim_dissimilarity = \"NA\"\n",
        "                                glcm_sim_homogeneity = \"NA\"\n",
        "                                glcm_sim_energy = \"NA\"\n",
        "                                glcm_sim_correlation = \"NA\"\n",
        "                                psnr2 = \"NA\"\n",
        "                                ssim_value2 = \"NA\"\n",
        "\n",
        "                            else:\n",
        "                                # Convert PIL images to NumPy arrays\n",
        "                                raw_image_array = np.array(raw_image)\n",
        "                                transformed_image_array = np.array(transformed_image)\n",
        "                                \n",
        "                                # Apply the comparison function\n",
        "                                psnr, ssim_value = compare_images.calculate_image_metrics(raw_image_array, transformed_image_array)\n",
        "                                psnr = float(psnr)\n",
        "                                ssim_value = float(ssim_value)\n",
        "\n",
        "                                # Calculate metrics from metrics.py\n",
        "                                texture_metrics_original = calculate_texture_features(image = cv2.imread(temp_image_path))\n",
        "                                texture_metrics_transformed = calculate_texture_features(image = cv2.imread(transformed_image_path))\n",
        "                                similarity_metrics = calculate_similarity_metrics(texture_metrics_original, texture_metrics_transformed)\n",
        "                                image_metrics = calculate_image_metrics(cv2.imread(temp_image_path), cv2.imread(transformed_image_path))\n",
        "\n",
        "                                LBP_sim = similarity_metrics['lbp_similarity']\n",
        "                                glcm_sim_contrast = similarity_metrics['glcm_similarities']['contrast']\n",
        "                                glcm_sim_dissimilarity = similarity_metrics['glcm_similarities']['dissimilarity']\n",
        "                                glcm_sim_homogeneity = similarity_metrics['glcm_similarities']['homogeneity']\n",
        "                                glcm_sim_energy = similarity_metrics['glcm_similarities']['energy']\n",
        "                                glcm_sim_correlation = similarity_metrics['glcm_similarities']['correlation']\n",
        "                                psnr2 = image_metrics[0]\n",
        "                                ssim_value2 = image_metrics[1]\n",
        "\n",
        "\n",
        "                            # Write the result to the CSV file\n",
        "                            csv_writer.writerow([image_name, folder, psnr, ssim_value, \n",
        "                                                LBP_sim, glcm_sim_contrast, glcm_sim_dissimilarity,\n",
        "                                                glcm_sim_homogeneity, glcm_sim_energy,\n",
        "                                                glcm_sim_correlation, psnr2, ssim_value2,\n",
        "                                                detected_bits_raw, detected_bits_transformed, watermark_present])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_08cc23c13b79af4d3852e78a8af8ced_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "0_126b1334283b521949e0684339c5389b_original_trustmark_watermarked.png\n",
            "Processed 2 images, stopping further processing.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from metrics import calculate_texture_features, calculate_similarity_metrics ,calculate_image_metrics\n",
        "import cv2\n",
        "\n",
        "import importlib.util\n",
        "\n",
        "##Below code paragraph imports compare_images.py from the specified path\n",
        "module_path = home_dir + '/utils/compare_images.py'\n",
        "spec = importlib.util.spec_from_file_location(\"compare_images\", module_path)\n",
        "compare_images = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(compare_images)\n",
        "\n",
        "# Define the paths\n",
        "\n",
        "transformed_folders = [\n",
        "                        \"brightened_images0-5\",\n",
        "                        \"brightened_images0-7\",\n",
        "                        \"brightened_images0-8\",\n",
        "                        \"brightened_images0-9\",\n",
        "                        \"brightened_images1\",\n",
        "                        \"brightened_images1-1\",\n",
        "                        \"brightened_images1-2\",\n",
        "                        \"brightened_images1-3\",\n",
        "                        \"brightened_images1-5\",\n",
        "                        \"brightened_images1-75\",\n",
        "                        \"contrast_images0-5\",\n",
        "                        \"contrast_images0-7\",\n",
        "                        \"contrast_images0-8\",\n",
        "                        \"contrast_images0-9\",\n",
        "                        \"contrast_images1\",\n",
        "                        \"contrast_images1-1\",\n",
        "                        \"contrast_images1-2\",\n",
        "                        \"contrast1-3\",\n",
        "                        \"contrast_images1-5\",\n",
        "                        \"contrast_images1-75\",\n",
        "                        \"gamma_images0-5\",\n",
        "                        \"gamma_images0-7\",\n",
        "                        \"gamma_images0-8\",\n",
        "                        \"gamma_images0-9\",\n",
        "                        \"gamma_images1\",\n",
        "                        \"gamma_images1-1\",\n",
        "                        \"gamma_images1-2\",\n",
        "                        \"gamma_images1-3\",\n",
        "                        \"gamma_images1-5\",\n",
        "                        \"gamma_images1-75\",\n",
        "                        \"hue_imagesminus0-5\",\n",
        "                        \"hue_imagesminus0-3\",\n",
        "                        \"hue_imagesminus0-2\",\n",
        "                        \"hue_imagesminus0-1\",\n",
        "                        \"hue_images0\",\n",
        "                        \"hue_images0-1\",\n",
        "                        \"hue_images0-2\",\n",
        "                        \"hue_images0-3\",\n",
        "                        \"hue_images0-5\",\n",
        "                        \"saturation_images0-25\",\n",
        "                        \"saturation_images0-5\",\n",
        "                        \"saturation_images0-7\",\n",
        "                        \"saturation_images0-8\",\n",
        "                        \"saturation_images0-9\",\n",
        "                        \"saturation_images1\",\n",
        "                        \"saturation_images1-1\",\n",
        "                        \"saturation_images1-2\",\n",
        "                        \"saturation_images1-3\",\n",
        "                        \"saturation_images1-5\",\n",
        "                        \"saturation_images1-75\",\n",
        "                        \"sharpness_images0-25\",\n",
        "                        \"sharpness_images0-5\",\n",
        "                        \"sharpness_images0-7\",\n",
        "                        \"sharpness_images0-8\",\n",
        "                        \"sharpness_images0-9\",\n",
        "                        \"sharpness_images1\",\n",
        "                        \"sharpness_images1-1\",\n",
        "                        \"sharpness_images1-2\",\n",
        "                        \"sharpness_images1-3\",\n",
        "                        \"sharpness_images1-5\",\n",
        "                        \"sharpness_images1-75\",\n",
        "                      ]  \n",
        "\n",
        "transformed_folders_paths = [os.path.join(download_path, folder) for folder in transformed_folders]\n",
        "\n",
        "# Define the output CSV file\n",
        "output_csv_path = os.path.join(download_path, \"comparison_results_brightcongamma_deepdivefullsuite.csv\")\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open(output_csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)\n",
        "    \n",
        "    # Write the header row\n",
        "    header = [\"Image Name\", \"Transform\", \"PSNR\", \"SSIM_value\", \n",
        "                \"LBP_sim\", \"glcm_sim_contrast\", \"glcm_sim_dissimilarity\",\n",
        "                \"glcm_sim_homogeneity\", \"glcm_sim_energy\",\n",
        "                \"glcm_sim_correlation\", \"psnr2\", \"ssim_value2\",\n",
        "                \"Watermark_raw\", \"Watermark_transformed\", \"Watermark_present\"]  # Add more columns for additional placeholder functions\n",
        "    csv_writer.writerow(header)\n",
        "    \n",
        "    row_counter = 1\n",
        "    image_counter = 0\n",
        "    # Loop through all images in the raw images folder\n",
        "    for image_name in os.listdir(raw_images_path):\n",
        "        temp_image_path = os.path.join(raw_images_path, image_name)\n",
        "        if row_counter % 10 == 0:\n",
        "            print(f\"Processing image {row_counter}\")\n",
        "        row_counter += 1\n",
        "        if image_counter >= images_to_process + 2:\n",
        "            print(f\"Processed {images_to_process} images, stopping further processing.\")\n",
        "            break  \n",
        "        image_counter = image_counter + 1\n",
        "        \n",
        "        #print(\"a\", row_counter)\n",
        "        # Check if the file is an image\n",
        "        if os.path.isfile(temp_image_path) and image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            # Open the raw image\n",
        "            #print(\"b\", row_counter)\n",
        "            with Image.open(temp_image_path) as raw_image:\n",
        "                # Loop through each transformed folder\n",
        "                for folder, folder_path in zip(transformed_folders, transformed_folders_paths):\n",
        "                    transformed_image_path = os.path.join(folder_path, image_name)\n",
        "                    #print(\"in transformed loop\", row_counter, transformed_image_path)\n",
        "                    # Check if the transformed image exists\n",
        "                    if os.path.isfile(transformed_image_path):\n",
        "                        #print(\"in transformed i exists\")\n",
        "                        with Image.open(transformed_image_path) as transformed_image:\n",
        "                            #print(\"in transformed image\", row_counter)\n",
        "                            # Detect if there is a watermark in the raw image\n",
        "                            detected_bits_raw = detect_watermark(temp_image_path, ckpt_path)\n",
        "\n",
        "                            # Detect if there is a watermark in the transformed image\n",
        "                            detected_bits_transformed = detect_watermark(transformed_image_path, ckpt_path)\n",
        "\n",
        "                            if (detected_bits_raw == detected_bits_transformed):\n",
        "                                watermark_present = 1\n",
        "                            else:\n",
        "                                watermark_present = 0\n",
        "\n",
        "                            # Check if the images are the same size\n",
        "                            if raw_image.size != transformed_image.size:\n",
        "                                psnr = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                ssim_value = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                LBP_sim = \"NA\"\n",
        "                                glcm_sim_contrast = \"NA\"\n",
        "                                glcm_sim_dissimilarity = \"NA\"\n",
        "                                glcm_sim_homogeneity = \"NA\"\n",
        "                                glcm_sim_energy = \"NA\"\n",
        "                                glcm_sim_correlation = \"NA\"\n",
        "                                psnr2 = \"NA\"\n",
        "                                ssim_value2 = \"NA\"\n",
        "\n",
        "                            else:\n",
        "                                # Convert PIL images to NumPy arrays\n",
        "                                raw_image_array = np.array(raw_image)\n",
        "                                transformed_image_array = np.array(transformed_image)\n",
        "                                \n",
        "                                # Apply the comparison function\n",
        "                                psnr, ssim_value = compare_images.calculate_image_metrics(raw_image_array, transformed_image_array)\n",
        "                                psnr = float(psnr)\n",
        "                                ssim_value = float(ssim_value)\n",
        "\n",
        "                                # Calculate metrics from metrics.py\n",
        "                                texture_metrics_original = calculate_texture_features(image = cv2.imread(temp_image_path))\n",
        "                                texture_metrics_transformed = calculate_texture_features(image = cv2.imread(transformed_image_path))\n",
        "                                similarity_metrics = calculate_similarity_metrics(texture_metrics_original, texture_metrics_transformed)\n",
        "                                image_metrics = calculate_image_metrics(cv2.imread(temp_image_path), cv2.imread(transformed_image_path))\n",
        "\n",
        "                                LBP_sim = similarity_metrics['lbp_similarity']\n",
        "                                glcm_sim_contrast = similarity_metrics['glcm_similarities']['contrast']\n",
        "                                glcm_sim_dissimilarity = similarity_metrics['glcm_similarities']['dissimilarity']\n",
        "                                glcm_sim_homogeneity = similarity_metrics['glcm_similarities']['homogeneity']\n",
        "                                glcm_sim_energy = similarity_metrics['glcm_similarities']['energy']\n",
        "                                glcm_sim_correlation = similarity_metrics['glcm_similarities']['correlation']\n",
        "                                psnr2 = image_metrics[0]\n",
        "                                ssim_value2 = image_metrics[1]\n",
        "                                #print(\"in metric\", row_counter)\n",
        "\n",
        "\n",
        "                            # Write the result to the CSV file\n",
        "                            print(image_name)\n",
        "                            csv_writer.writerow([image_name, folder, psnr, ssim_value, \n",
        "                                                LBP_sim, glcm_sim_contrast, glcm_sim_dissimilarity,\n",
        "                                                glcm_sim_homogeneity, glcm_sim_energy,\n",
        "                                                glcm_sim_correlation, psnr2, ssim_value2,\n",
        "                                                detected_bits_raw, detected_bits_transformed, watermark_present])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scratch code.\n",
        "This section includes 'scratch' code not for production.\n",
        "The most important section is the section designed for testing screenshots\n",
        "Note that you will need to manually take screenshots of ~10 images in the dataset, and save them to the relevant location before running this code.\n",
        "To stop the code being run automatically, we have set up a \"if (0==1)\" statement. Change that to \"1=1\" before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (0==1):\n",
        "\n",
        "    print(\"LBP_similarity\", similarity_metrics['lbp_similarity'])\n",
        "    print(\"glcm_similarities_contrast\", similarity_metrics['glcm_similarities']['contrast'])\n",
        "\n",
        "    print(\"glcm_similarities_dissimilarity\", similarity_metrics['glcm_similarities']['dissimilarity'])\n",
        "    print(\"glcm_similarities_homogeneity\", similarity_metrics['glcm_similarities']['homogeneity'])\n",
        "    print(\"glcm_similarities_energy\", similarity_metrics['glcm_similarities']['energy'])\n",
        "    print(\"glcm_similarities_correlation\", similarity_metrics['glcm_similarities']['correlation'])\n",
        "\n",
        "    print(\"image metrics\", image_metrics)\n",
        "    print(\"psnr\", image_metrics[0])\n",
        "    print(\"ssim_value\", image_metrics[1])\n",
        "\n",
        "    '''\n",
        "    LBP_sim\n",
        "    glcm_sim_contrast\n",
        "    glcm_sim_dissimilarity\n",
        "    glcm_sim_homogeneity\n",
        "    glcm_sim_energy\n",
        "    glcm_sim_correlation\n",
        "    psnr2\n",
        "    ssim_value2\n",
        "\n",
        "\n",
        "    similarity_metrics['lbp_similarity']\n",
        "    similarity_metrics['glcm_similarities']['contrast']\n",
        "    similarity_metrics['glcm_similarities']['dissimilarity']\n",
        "    similarity_metrics['glcm_similarities']['homogeneity']\n",
        "    similarity_metrics['glcm_similarities']['energy']\n",
        "    similarity_metrics['glcm_similarities']['correlation']\n",
        "    image_metrics[0]\n",
        "    image_metrics[1]\n",
        "\n",
        "    '''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Scratch code\n",
        "##\n",
        "##\n",
        "## This is a one-off script to compare Halla's screenshots with the original images\n",
        "\n",
        "if (0==1):\n",
        "\n",
        "    import os\n",
        "    import csv\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    from metrics import calculate_texture_features, calculate_similarity_metrics ,calculate_image_metrics\n",
        "    import cv2\n",
        "\n",
        "    import importlib.util\n",
        "\n",
        "    ##Below code paragraph imports compare_images.py from the specified path\n",
        "    module_path = home_dir + '/utils/compare_images.py'\n",
        "    spec = importlib.util.spec_from_file_location(\"compare_images\", module_path)\n",
        "    compare_images = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(compare_images)\n",
        "\n",
        "    # Define a placeholder function\n",
        "    def compare(image1, image2):\n",
        "        \"\"\"Placeholder function to compare two images and return a number.\"\"\"\n",
        "        # Replace this with your actual comparison logic\n",
        "        return 0.0\n",
        "\n",
        "    # Define the paths\n",
        "    raw_images_path = os.path.join(download_path, \"raw_watermarked_images\")\n",
        "    transformed_folders = [\n",
        "                            \"screenshotted\"\n",
        "                            ]    \n",
        "\n",
        "    transformed_folders_paths = [os.path.join(download_path, folder) for folder in transformed_folders]\n",
        "\n",
        "    # Define the output CSV file\n",
        "    output_csv_path = os.path.join(download_path, \"comparison_results_screenshotted.csv\")\n",
        "\n",
        "    # Open the CSV file for writing\n",
        "    with open(output_csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        \n",
        "        # Write the header row\n",
        "        header = [\"Image Name\", \"Transform\", \"PSNR\", \"SSIM_value\", \n",
        "                    \"LBP_sim\", \"glcm_sim_contrast\", \"glcm_sim_dissimilarity\",\n",
        "                    \"glcm_sim_homogeneity\", \"glcm_sim_energy\",\n",
        "                    \"glcm_sim_correlation\", \"psnr2\", \"ssim_value2\",\n",
        "                    \"Watermark_raw\", \"Watermark_transformed\", \"Watermark_present\"]  # Add more columns for additional placeholder functions\n",
        "        csv_writer.writerow(header)\n",
        "        \n",
        "        row_counter = 1\n",
        "        # Loop through all images in the raw images folder\n",
        "        for image_name in os.listdir(raw_images_path):\n",
        "            raw_image_path = os.path.join(raw_images_path, image_name)\n",
        "            if row_counter % 10 == 0:\n",
        "                print(f\"Processing image {row_counter}\")\n",
        "            row_counter += 1\n",
        "            # Check if the file is an image\n",
        "            if os.path.isfile(raw_image_path) and image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                # Open the raw image\n",
        "                with Image.open(raw_image_path) as raw_image:\n",
        "                    # Loop through each transformed folder\n",
        "                    for folder, folder_path in zip(transformed_folders, transformed_folders_paths):\n",
        "                        transformed_image_path = os.path.join(folder_path, image_name)\n",
        "                        \n",
        "                        # Check if the transformed image exists\n",
        "                        if os.path.isfile(transformed_image_path):\n",
        "                            with Image.open(transformed_image_path) as transformed_image:\n",
        "                                \n",
        "                                \n",
        "                                # Detect if there is a watermark in the raw image\n",
        "                                detected_bits_raw = detect_watermark(raw_image_path, ckpt_path)\n",
        "\n",
        "                                # Detect if there is a watermark in the transformed image\n",
        "                                detected_bits_transformed = detect_watermark(transformed_image_path, ckpt_path)\n",
        "\n",
        "                                if (detected_bits_raw == detected_bits_transformed):\n",
        "                                    watermark_present = 1\n",
        "                                else:\n",
        "                                    watermark_present = 0\n",
        "\n",
        "                                # Check if the images are the same size\n",
        "                                if raw_image.size != transformed_image.size:\n",
        "                                    psnr = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                    ssim_value = \"NA\"  # Return 'NA' if sizes are different\n",
        "                                    LBP_sim = \"NA\"\n",
        "                                    glcm_sim_contrast = \"NA\"\n",
        "                                    glcm_sim_dissimilarity = \"NA\"\n",
        "                                    glcm_sim_homogeneity = \"NA\"\n",
        "                                    glcm_sim_energy = \"NA\"\n",
        "                                    glcm_sim_correlation = \"NA\"\n",
        "                                    psnr2 = \"NA\"\n",
        "                                    ssim_value2 = \"NA\"\n",
        "\n",
        "                                else:\n",
        "                                    # Convert PIL images to NumPy arrays\n",
        "                                    raw_image_array = np.array(raw_image)\n",
        "                                    transformed_image_array = np.array(transformed_image)\n",
        "                                    \n",
        "                                    # Apply the comparison function\n",
        "                                    psnr, ssim_value = compare_images.calculate_image_metrics(raw_image_array, transformed_image_array)\n",
        "                                    psnr = float(psnr)\n",
        "                                    ssim_value = float(ssim_value)\n",
        "\n",
        "                                    # Calculate metrics from metrics.py\n",
        "                                    texture_metrics_original = calculate_texture_features(image = cv2.imread(raw_image_path))\n",
        "                                    texture_metrics_transformed = calculate_texture_features(image = cv2.imread(transformed_image_path))\n",
        "                                    similarity_metrics = calculate_similarity_metrics(texture_metrics_original, texture_metrics_transformed)\n",
        "                                    image_metrics = calculate_image_metrics(cv2.imread(raw_image_path), cv2.imread(transformed_image_path))\n",
        "\n",
        "                                    LBP_sim = similarity_metrics['lbp_similarity']\n",
        "                                    glcm_sim_contrast = similarity_metrics['glcm_similarities']['contrast']\n",
        "                                    glcm_sim_dissimilarity = similarity_metrics['glcm_similarities']['dissimilarity']\n",
        "                                    glcm_sim_homogeneity = similarity_metrics['glcm_similarities']['homogeneity']\n",
        "                                    glcm_sim_energy = similarity_metrics['glcm_similarities']['energy']\n",
        "                                    glcm_sim_correlation = similarity_metrics['glcm_similarities']['correlation']\n",
        "                                    psnr2 = image_metrics[0]\n",
        "                                    ssim_value2 = image_metrics[1]\n",
        "\n",
        "\n",
        "                                # Write the result to the CSV file\n",
        "                                csv_writer.writerow([image_name, folder, psnr, ssim_value, \n",
        "                                                    LBP_sim, glcm_sim_contrast, glcm_sim_dissimilarity,\n",
        "                                                    glcm_sim_homogeneity, glcm_sim_energy,\n",
        "                                                    glcm_sim_correlation, psnr2, ssim_value2,\n",
        "                                                    detected_bits_raw, detected_bits_transformed, watermark_present])"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "azureml_py38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
